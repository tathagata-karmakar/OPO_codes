#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Jul 31 09:22:51 2023

@author: t_karmakar
"""

import os
import qutip as qt
import time
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
from scipy.integrate import simps as intg
from matplotlib import rc
from pylab import rcParams
#from FNO_structure import *
os.environ["PATH"] += os.pathsep + '/Library/TeX/texbin'
rc('text',usetex=True)

import jax
import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random
from jax import lax
from jax import device_put
from jax import make_jaxpr
from jax.scipy.special import logsumexp
from functools import partial
import collections 
from typing import Iterable

import tensorflow as tf
tf.config.set_visible_devices([],device_type='GPU')
import tensorflow_datasets as tfds

from torch.utils import data
from torchvision.datasets import MNIST

def random_layer_params(m,n,key,scale=1e-2):
    w_key,b_key=random.split(key)
    return scale*random.normal(w_key,(n,m)),scale*random.normal(b_key,(n,))
def init_network_params(sizes,key):
    keys=random.split(key,len(sizes))
    return [random_layer_params(m,n,k) for m,n,k in zip(sizes[:-1],sizes[1:],keys)]

def relu(x):
    return jnp.maximum(0,x)

def predict(params, image):
    activations =image
    for w,b in params[:-1]:
        outputs = jnp.dot(w,activations)+b
        activations = relu(outputs)
    final_w,final_b=params[-1]
    logits=jnp.dot(final_w,activations)+final_b
    return logits - logsumexp(logits)

def one_hot(x,k,dtype=jnp.float32):
    return jnp.array(x[:, None] == jnp.arange(k),dtype)

def accuracy(params, images, targets):
    target_class = jnp.argmax(targets,axis=1)
    predicted_class = jnp.argmax(batched_predict(params,images),axis=1)
    return jnp.mean(predicted_class == target_class)

def loss(params, images, targets):
    preds = batched_predict(params, images)
    return -jnp.mean(preds * targets)

@jit 
def update(params, x, y):
    grads = grad(loss)(params, x, y)
    return [(w-step_size*dw,b-step_size*db) for (w,b), (dw, db) in zip(params, grads)]

layer_sizes=[784,512,512,10]
step_size=0.01
num_epochs=10
batch_size=128
n_targets=10
params=init_network_params(layer_sizes,random.PRNGKey(0))

batched_predict = vmap(predict,in_axes=(None,0))

'''
data_dir = '/tmp/tfds'

mnist_data, info = tfds.load(name="mnist",batch_size=-1, data_dir= data_dir, with_info=True)
mnist_data=tfds.as_numpy(mnist_data)
train_data,test_data=mnist_data['train'],mnist_data['test']
num_labels=info.features['label'].num_classes
h,w,c=info.features['image'].shape
num_pixels=h*w*c

train_images,train_labels = train_data['image'],train_data['label']
train_images = jnp.reshape(train_images,(len(train_images), num_pixels))
train_labels = one_hot(train_labels, num_labels)

test_images, test_labels = test_data['image'],test_data['label']
test_images = jnp.reshape(test_images,(len(test_images), num_pixels))
test_labels = one_hot(test_labels, num_labels)

print('Train:', train_images.shape, train_labels.shape)
print('Test:', test_images.shape, test_labels.shape)
'''


def numpy_collate(batch):
  if isinstance(batch[0], np.ndarray):
    return np.stack(batch)
  elif isinstance(batch[0], (tuple,list)):
    transposed = zip(*batch)
    return [numpy_collate(samples) for samples in transposed]
  else:
    return np.array(batch)

class NumpyLoader(data.DataLoader):
  def __init__(self, dataset, batch_size=1,
                shuffle=False, sampler=None,
                batch_sampler=None, num_workers=0,
                pin_memory=False, drop_last=False,
                timeout=0, worker_init_fn=None):
    super(self.__class__, self).__init__(dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        sampler=sampler,
        batch_sampler=batch_sampler,
        num_workers=num_workers,
        collate_fn=numpy_collate,
        pin_memory=pin_memory,
        drop_last=drop_last,
        timeout=timeout,
        worker_init_fn=worker_init_fn)

class FlattenAndCast(object):
  def __call__(self, pic):
    return np.ravel(np.array(pic, dtype=jnp.float32))

# Define our dataset, using torch datasets
mnist_dataset = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())
training_generator = NumpyLoader(mnist_dataset, batch_size=batch_size, num_workers=0)


# Get the full train dataset (for checking accuracy while training)
train_images = np.array(mnist_dataset.train_data).reshape(len(mnist_dataset.train_data), -1)
train_labels = one_hot(np.array(mnist_dataset.train_labels), n_targets)

# Get full test dataset
mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False)
test_images = jnp.array(mnist_dataset_test.test_data.numpy().reshape(len(mnist_dataset_test.test_data), -1), dtype=jnp.float32)
test_labels = one_hot(np.array(mnist_dataset_test.test_labels), n_targets)

'''
def get_train_batches():
    ds = tfds.load(name='mnist',split='train', as_supervised='True',data_dir=data_dir)
    ds = ds.batch(batch_size).prefetch(1)
    return tfds.as_numpy(ds)

for epoch in range(num_epochs):
    start_time = time.time()
    for x, y in get_train_batches():
        x = jnp.reshape(x, (len(x), num_pixels))
        y = one_hot(y, num_labels)
        params = update(params, x, y)
    epoch_time = time.time() - start_time
    
    train_acc = accuracy(params, train_images, train_labels)
    test_acc = accuracy(params, test_images, test_labels)
    print("Epoch {} in {:0.2f} sec".format(epoch,epoch_time))
    print("Training set accuracy {}".format(train_acc))
    print("Test set accuracy {}".format(test_acc))
'''

#import time

for epoch in range(num_epochs):
  start_time = time.time()
  for x, y in training_generator:
    y = one_hot(y, n_targets)
    params = update(params, x, y)
  epoch_time = time.time() - start_time

  train_acc = accuracy(params, train_images, train_labels)
  test_acc = accuracy(params, test_images, test_labels)
  print("Epoch {} in {:0.2f} sec".format(epoch, epoch_time))
  print("Training set accuracy {}".format(train_acc))
  print("Test set accuracy {}".format(test_acc))


